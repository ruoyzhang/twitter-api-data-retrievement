{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototyping a twitter scraper that scrapes tweets:\n",
    "### 1. from the results of advanced search queries\n",
    "### 2. from users localised to be in the geographical region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import shutil\n",
    "import urllib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import selenium.webdriver.support.ui as ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from math import floor\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "# new ones\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing my credencials\n",
    "username = 'xxxxxxx'\n",
    "password = 'xxxxxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating the driver\n",
    "driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redirecting to the login interface\n",
    "driver.get('https://twitter.com/login?lang=en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding input fields\n",
    "usn = driver.find_element_by_class_name(\"js-username-field\")\n",
    "pwd = driver.find_element_by_class_name(\"js-password-field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authenticating\n",
    "usn.send_keys(username)\n",
    "pwd.send_keys(password)\n",
    "driver.find_element_by_class_name(\"EdgeButtom--medium\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redirecting to the advanced search page\n",
    "driver.get('https://twitter.com/search?l=en&q=%23AI%20OR%20%23artificialintelligence%20near%3A%22Paris%2C%20France%22%20within%3A1500mi%20since%3A2019-04-03%20until%3A2019-04-04&src=typd&lang=en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrooooooolllllllll\n",
    "for i in range(50):\n",
    "    driver.execute_script(\"window.scrollTo(0, 1000000)\")\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the original tweets\n",
    "all_tweets = driver.find_elements_by_xpath(\"//p[@class='TweetTextSize  js-tweet-text tweet-text']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in all_tweets:\n",
    "    print(tweet.text)\n",
    "    print('------------------------------------------')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the dates\n",
    "all_dates = driver.find_elements_by_xpath(\"//a[@class='tweet-timestamp js-permalink js-nav js-tooltip']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in all_dates:\n",
    "    print(date.text)\n",
    "    print('------------------------------------------')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting tweek ids in order to facilitate deduplication\n",
    "dropdowns = driver.find_elements_by_xpath(\"//button[@class='ProfileTweet-actionButton u-textUserColorHover dropdown-toggle js-dropdown-toggle']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dropdowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting tweet ids\n",
    "tweet_id_links = []\n",
    "for i, dropdown in enumerate(dropdowns):\n",
    "    dropdown.click()\n",
    "    driver.find_elements_by_xpath(\n",
    "        \"//li[@class='copy-link-to-tweet js-actionCopyLinkToTweet']/button[@class='dropdown-link' and @role='menuitem']\")[i].click()\n",
    "    tweet_id_links.append(pyperclip.paste())\n",
    "    print('iteration {} is done'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tweet_id_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now capable of capturing all 3 fields' data, we now move onto study how we can formulate the changing of dates, which should be simple in comparison with Weibo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now test the variables\n",
    "lang = 'en'\n",
    "hash_tags = 'AI, artificialintelligence, machinelearning'\n",
    "geocenter = 'Paris, France'\n",
    "radius = 2500\n",
    "unit = 'km'\n",
    "since = '2016-04-21'\n",
    "until = '2016-04-22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parrsing where necessary\n",
    "hash_tag_sequence = ['%23'+TG.strip() for TG in hash_tags.split(',')]\n",
    "hash_tag_sequence = \"%20OR%20\".join(hash_tag_sequence)\n",
    "\n",
    "city = geocenter.split(',')[0].strip()\n",
    "country = geocenter.split(',')[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forming query\n",
    "query = \"https://twitter.com/search?l={}&q={}%20near%3A%22{}%2C%20{}%22%20within%3A{}{}%20since%3A{}%20until%3A{}&src=typd&lang=en\".format(\n",
    "    lang, hash_tag_sequence, city, country, radius, unit, since, until)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "driver.get(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dis werkz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're screwed up by twitter we are not capable of getting a good quantity of historical tweets using the advanced search function, we might need to find a work around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
