{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this section we test out getting a bearer token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OAUTH2_TOKEN = 'https://api.twitter.com/oauth2/token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bearer_token(consumer_key, consumer_secret):\n",
    "    # enconde consumer key\n",
    "    consumer_key = urllib.parse.quote(consumer_key)\n",
    "    # encode consumer secret\n",
    "    consumer_secret = urllib.parse.quote(consumer_secret)\n",
    "    # create bearer token\n",
    "    bearer_token = consumer_key + ':' + consumer_secret\n",
    "    # base64 encode the token\n",
    "    base64_encoded_bearer_token = base64.b64encode(bearer_token.encode('utf-8'))\n",
    "    # set headers\n",
    "    headers = {\n",
    "        \"Authorization\": \"Basic \" + base64_encoded_bearer_token.decode('utf-8') + \"\",\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded;charset=UTF-8\",\n",
    "        \"Content-Length\": \"29\"}\n",
    "\n",
    "    response = requests.post(OAUTH2_TOKEN, headers=headers, data={'grant_type': 'client_credentials'})\n",
    "    to_json = response.json()\n",
    "    print(\"token_type = %s\\naccess_token  = %s\" % (to_json['token_type'], to_json['access_token']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'insert_consumer_key_here'\n",
    "consumer_secret = 'insert_consumer_secret_here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_type = bearer\n",
      "access_token  = AAAAAAAAAAAAAAAAAAAAAN%2FX%2BAAAAAAAE6nRL3awxuYlmfDQthJspTHHbQw%3Dr0u7om0T7JuX2RK9JgBa4aegOZQtyc2q9aAL6OfDLpXc76rjxt\n"
     ]
    }
   ],
   "source": [
    "get_bearer_token(consumer_key, consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\":\".join([consumer_key, consumer_secret]) == consumer_key+':'+consumer_secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now that we have the bearer token, we can move to try out making queries for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchtweets import ResultStream, gen_rule_payload, load_credentials, collect_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grabbing bearer token from OAUTH\n"
     ]
    }
   ],
   "source": [
    "# setting the authenication methods for data\n",
    "\n",
    "premium_search_data = load_credentials(filename=\"~/.twitter_keys.yaml\",\n",
    "                                       yaml_key = 'search_tweets_api_data',\n",
    "                                       env_overwrite = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\":\"artificial intelligence bounding_box:[-16.17 36.45 15.21 36.45]\",\"maxResults\":100,\"toDate\":\"201604180000\",\"fromDate\":\"201604170000\"}\n"
     ]
    }
   ],
   "source": [
    "# setting the rule for both count and data\n",
    "rule = gen_rule_payload(\"artificial intelligence bounding_box:[-16.17 36.45 15.21 36.45]\",\n",
    "                        from_date=\"2016-04-17\",\n",
    "                        to_date=\"2016-04-18\",\n",
    "                        results_per_call=100)\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "retrying request; current status code: 422\n",
      "retrying request; current status code: 422\n",
      "retrying request; current status code: 422\n",
      "HTTP Error code: 422: There were errors processing your request: Width of bounding_box must be less than 25 miles (at position 25)\n",
      "Rule payload: {'query': 'artificial intelligence bounding_box:[-16.17 36.45 15.21 36.45]', 'maxResults': 100, 'toDate': '201604180000', 'fromDate': '201604170000'}\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-95c7c71d7326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_stream_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpremium_search_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/searchtweets/result_stream.py\u001b[0m in \u001b[0;36mcollect_results\u001b[0;34m(rule, max_results, result_stream_args)\u001b[0m\n\u001b[1;32m    306\u001b[0m                       \u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                       **result_stream_args)\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/searchtweets/result_stream.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/searchtweets/result_stream.py\u001b[0m in \u001b[0;36mexecute_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m         resp = request(session=self.session,\n\u001b[1;32m    259\u001b[0m                        \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                        rule_payload=self.rule_payload)\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_requests\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mResultStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_request_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/searchtweets/result_stream.py\u001b[0m in \u001b[0;36mretried_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HTTP Error code: {}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rule payload: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rule_payload\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tweets = collect_results(rule, result_stream_args = premium_search_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------0----------\n",
      "Professions at Immediate #Risk From #Automation: https://t.co/KiB5GpYQSh\n",
      "#DigitalTransformation #innovation #bidgata #defstar5 #mpgvip #tech https://t.co/0HuREavLNr\n",
      "----------1----------\n",
      "Artificial intelligence: Is this the future of early cancer detection? via @bioengineers https://t.co/vfDwNRgr4e\n",
      "----------2----------\n",
      "Google and Amazon are spearheading a quiet gadget revolution, and it's bad news for Apple https://t.co/e4SeiHEETf https://t.co/ZGCDTfFQaW\n",
      "----------3----------\n",
      "Alex jones really just waterboarded an iPad to get information from the artificial intelligence he obviously knows something we dont\n",
      "----------4----------\n",
      "Artificial intelligence smart enough to fool Captcha security check\n",
      "\n",
      " Image copyright Getty Images \n",
      "Computer scientists ...\n",
      " \n",
      "#14UM_NET https://t.co/i4MupHWUmn\n",
      "----------5----------\n",
      "Shelley, the AI writer, is scarily good at writing horror. https://t.co/a6SltQZFKx\n",
      "----------6----------\n",
      "The connection between data integration and artificial intelligence is growing https://t.co/kKbudNwwai \n",
      "\n",
      "(by @BarryDevlin of @TDWI) #AI\n",
      "----------7----------\n",
      "Artificial Intelligence, Andela and Africa’s looming technology slavery https://t.co/GUcjpssZlL\n",
      "----------8----------\n",
      "Artificial Intelligence: An Integrated Innovation (Part II) — CoverShr https://t.co/TxBiQ8eH3T #ai #ml #dl\n",
      "----------9----------\n",
      "Toyota wants to get us truly crushing on cars https://t.co/x5GAAHt4gq #Artificial_Intelligence #Automotive #TC #Transportation #tech https://t.co/5RJZd2Hnju\n"
     ]
    }
   ],
   "source": [
    "for i, tweet in enumerate(tweets[0:10]):\n",
    "    print('----------{}----------'.format(i))\n",
    "    print(tweet.all_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
